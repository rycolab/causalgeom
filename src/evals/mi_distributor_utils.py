import warnings
import logging
import os
import sys
import coloredlogs
import argparse

import numpy as np
import torch
import random 
from scipy.special import softmax, log_softmax

sys.path.append('..')

from data.filter_generations import load_filtered_generations
from utils.lm_loaders import GPT2_LIST, get_max_cxt_length
from paths import OUT

#########################################
# Arg Handling                         #
#########################################
def get_nucleus_arg(source):
    """ Use samples generated by model with nucleus sampling 
    for natural text (test set) samples.
    """
    if source in ["gen_nucleus_concept", "gen_nucleus_all"]:
        nucleus = True
    elif source in ["gen_ancestral_concept", "gen_ancestral_all"]:
        nucleus = False
    elif source in ["natural_all", "natural_concept"]:
        #TODO: THIS SHOULDNT BE IN HERE ITS A SHORTCUT FOR CEBAB
        nucleus = False 
    else:
        raise ValueError(f"Incorrect {source} argument")
    return nucleus

#########################################
# Eval Directory Handling               #
#########################################
def get_mt_eval_directory(run_path, concept, model_name, 
    output_folder, source, iteration):
    rundir = os.path.dirname(run_path)
    rundir_name = os.path.basename(rundir)

    run_id = run_path[-27:-4]
    outdir = os.path.join(
        OUT, 
        f"mt_eval/{output_folder}/{concept}/{model_name}/"
        f"rundir_{rundir_name}/run_{run_id}/source_{source}/"
        f"evaliter_{iteration}"
    )
    return outdir

#########################################
# Data Handling                         #
#########################################
def get_all_hs(l0_gens, l1_gens, other_gens, max_all_hs, torch_dtype):
    """ Collects hs from filtered generations,
    stacks them and subsamples to max_all_hs samples.
    """
    l0_hs = [x for x,_,_,_ in l0_gens]
    l1_hs = [x for x,_,_,_ in l1_gens]
    other_hs = [x for x,_ in other_gens]

    all_hs = np.vstack(l0_hs + l1_hs + other_hs)
    del l0_hs
    del l1_hs
    del other_hs

    # subsamples all_hs to a max number to avoid memory issue
    if all_hs.shape[0] > max_all_hs:
        idx = np.arange(all_hs.shape[0])
        np.random.shuffle(idx)
        all_hs = all_hs[idx[:max_all_hs]]

    all_hs = torch.tensor(all_hs, dtype=torch_dtype)
    return all_hs

def sample_cxt_toks(cxt_toks, max_n_cxts):
    if len(cxt_toks) > max_n_cxts:
        cxt_toks = random.sample(
            cxt_toks, max_n_cxts)
    return cxt_toks

def get_cxt_toks(model_name, source, l0_gens, l1_gens, other_gens, 
    max_n_cxts, cxt_max_length_pct=1):
    """ Process and filters generated context strings
    """
    l0_cxt_toks = [all_tok[:-len(fact)] for _,fact,_,all_tok in l0_gens]
    l1_cxt_toks = [all_tok[:-len(fact)] for _,fact,_,all_tok in l1_gens]

    max_cxt_length = get_max_cxt_length(model_name)
    cxt_size_limit = max_cxt_length * cxt_max_length_pct

    concept_cxt_toks = [
        x for x in l0_cxt_toks + l1_cxt_toks if len(x) < cxt_size_limit
    ]
    del l0_cxt_toks
    del l1_cxt_toks
    if source in ["gen_ancestral_concept", "gen_nucleus_concept"]:
        concept_cxt_toks = sample_cxt_toks(concept_cxt_toks, max_n_cxts)
        return concept_cxt_toks
    elif source in ["gen_ancestral_all", "gen_nucleus_all"]:
        # process other cxt toks
        other_cxt_toks = [all_tok for _,all_tok in other_gens]
        other_cxt_toks = [
            x for x in other_cxt_toks if len(x) < cxt_size_limit
        ]

        # combine concept and other
        all_cxt_toks = concept_cxt_toks + other_cxt_toks
        del other_cxt_toks

        all_cxt_toks = sample_cxt_toks(all_cxt_toks, max_n_cxts)
        return all_cxt_toks
    else:
        raise NotImplementedError(
            f"Source {source} context tokens not implemented"
        )

def prep_generated_data(model_name, concept, nucleus, source, torch_dtype,
    cxt_max_length_pct, max_n_cxts, max_n_all_hs):
    """ Loads generated text and outputs:
    - all_hs: generated hs by the model
    - cxt_toks: either concept or all context tokens, depending on source arg
    
    Args:
    - cxt_max_length_pct: keep only cxt strings with cxt_max_length_pct
        length of model context size
    - max_n_cxts: max number of context strings to return,
                  applied to both concept_cxt_toks and all_cxt_toks
    - max_n_all_hs: max number of all_hs to return
    """
    l0_gens, l1_gens, other_gens = load_filtered_generations(
        model_name, concept, nucleus=nucleus
    )

    all_hs = get_all_hs(
        l0_gens, l1_gens, other_gens, max_n_all_hs, torch_dtype
    )    

    if source in ["train_all", "train_concept", "test_all", "test_concept"]:
        cxt_toks = None
        len_cxt_toks = 0
    else:
        cxt_toks = get_cxt_toks(
            model_name, source, l0_gens, l1_gens, other_gens, max_n_cxts,
            cxt_max_length_pct
        )
        len_cxt_toks = len(cxt_toks)

    logging.info(
        f"Loaded generated hs: model {model_name}, "
        f"concept {concept}, nucleus {nucleus}, "
        f"source {source}: \n"
        f"- all_hs: {all_hs.shape[0]} \n"
        f"- cxt_toks: {len_cxt_toks}"
    )
    return all_hs, cxt_toks

def pad_cxt_list(cxt_toks, max_nsamples, padding_value=-1):
    """ Input: list of tokenized strings
    Output: (nsamples, max_ntokens) dimensional tensor
    """
    if len(cxt_toks) > max_nsamples:
        cxt_toks = random.sample(cxt_toks, max_nsamples)
    torch_cxt_toks = [torch.tensor(x) for x in cxt_toks]
    padded_cxt_toks = torch.nn.utils.rnn.pad_sequence(
        torch_cxt_toks, padding_value=padding_value).T
    return padded_cxt_toks 

def filter_cxt_toks_by_length(model_name: str, cxt_toks: np.array, 
    cxt_max_length_pct: float, pad_token=-1) -> np.array:
    """ Filters an (n, d) dimensional array of cxt_toks to the subset
    of cxt_toks that have total non-pad tokens less than
    (model max context length) * cxt_max_length_pct
    """
    max_cxt_length = get_max_cxt_length(model_name)
    cxt_size_limit = int(max_cxt_length * cxt_max_length_pct)
    cxt_toks_notpad_count = (cxt_toks != -1).sum(1)
    cxt_toks_notpad_filter = cxt_toks_notpad_count < cxt_size_limit
    cxt_toks_sub = cxt_toks[cxt_toks_notpad_filter]
    return cxt_toks_sub

#########################################
# Past Key Values Handling              #
#########################################
def duplicate_pkv(pkv, num_repeats):
    pkv_dim = pkv[0][0].shape[0]
    if num_repeats > pkv_dim:
        return tuple(
            tuple(
                torch.cat([tensor] * num_repeats, dim=0) for tensor in layer
            ) for layer in pkv
        )
    else:
        return pkv

#########################################
# Distribution Computation              #
#########################################
def compute_log_pxh_batch(nntokH, V):
    logits = nntokH @ V.T
    log_pxnewh = torch.nn.functional.log_softmax(logits, dim=-1)
    return log_pxnewh

def compute_log_p_new_word(batch_log_pxh, new_word_tokens, seq_idxs, seq_lens, device):
    v = batch_log_pxh.shape[-1]
    v_mask = torch.isin(
        torch.arange(v), 
        torch.tensor(new_word_tokens), 
        assume_unique=True
    ).to(device)

    log_p_new = (batch_log_pxh + v_mask.log()).logsumexp(dim=-1)
    log_p_new_given_x = log_p_new[seq_idxs, seq_lens]
    return log_p_new_given_x

def compute_p_words(batch_token_list, batch_log_pxh, 
                    pad_token_id, new_word_tokens, device):
    """ 
    expected dimensions:
    - batch_token_list: bs x max_n_tokens
    - batch_pxh: bs x (max_n_tokens + 1) x |vocabulary|
    - new_word_tokens: list of new word tokens for the model

    output: list of word probabilities
    final_log_p_x: (bs)
    """
    seq_lens = (batch_token_list != pad_token_id).sum(1)
    n = len(batch_token_list)
    max_len = max(seq_lens)
    seq_idxs = torch.arange(n).to(device)
    log_p_x = torch.zeros(n).to(device)
    for i in range(max_len): # Could be vectorized further too if a bottleneck
        tok_idxs = batch_token_list[:, i]
        tok_pxhs = batch_log_pxh[seq_idxs, i, tok_idxs]
        log_p_x += tok_pxhs * (i < seq_lens)
    
    if new_word_tokens is not None:
        log_p_new_word = compute_log_p_new_word(
            batch_log_pxh, new_word_tokens, seq_idxs, seq_lens, device
        )
        final_log_p_x = log_p_x + log_p_new_word
        return final_log_p_x.exp().unsqueeze(0).cpu()
    else:
        return log_p_x.exp().unsqueeze(0).cpu()

def compute_m_log_p_new_word(batch_log_pxh, new_word_tokens, seq_lens, device):
    v = batch_log_pxh.shape[-1] # vocab size
    v_mask = torch.isin(
        torch.arange(v),
        torch.tensor(new_word_tokens),
        assume_unique=True
    ).to(device) # shape: (vocab_size), automatically broadcasts to (m, nwords, max_n_tokens + 1, vocab_size)

    bs_range = torch.arange(seq_lens.shape[0]).to(device)
    batch_log_pxh = batch_log_pxh[:, bs_range, seq_lens]
    log_p_new_given_x_proj = (batch_log_pxh + v_mask.log()).logsumexp(dim=-1)
    return log_p_new_given_x_proj

def compute_m_p_words(
    batch_tokens: torch.tensor, first_log_pxh:torch.tensor, 
    next_log_pxh: torch.tensor, pad_token_id: int, 
    new_word_tokens: list[int], device: str) -> torch.tensor:
    """ 
    expected dimensions:
    - batch_tokens: bs x max_n_tokens
    - first_log_pxh: msamples x V
    - next_log_pxh: bs x max_ntokens x V
    - pad_token_id: id of padding token in batch_tokens
    - new_word_tokens: list of new word tokens for the model

    output: list of word probabilities
    - final_log_p_x: msamples x bs
    """
    seq_lens = (batch_tokens != pad_token_id).sum(1)
    n = batch_tokens.shape[0]
    max_len = max(seq_lens)
    seq_idxs = torch.eye(n).to(device)
    bs_range = torch.arange(n).to(device)
    # m x bs
    log_p_x = first_log_pxh[:, batch_tokens[:, 0]]

    for i in range(1, max_len):
        tok_idxs = batch_tokens[:, i]
        next_pxhs = next_log_pxh[bs_range, i-1, tok_idxs]
        log_p_x += (next_pxhs * (i < seq_lens)).unsqueeze(0)

    if new_word_tokens is not None:
        log_p_new_given_x_proj = compute_m_log_p_new_word(
            next_log_pxh.unsqueeze(0), new_word_tokens, seq_lens-1, device
        )
        final_log_p_x = log_p_x + log_p_new_given_x_proj
        return final_log_p_x.exp()
    else:
        return log_p_x.exp()

#########################################
# Single Intervention Functions         #
#########################################
def apply_projection(hs, other_hs, mode, P, I_P):
    """ 
    hs: (m x d) 
    other_hs: (m x d)

    P: (d x d)
    I_P: (d x d)

    # rank(I-P)=d-1: project to H_bot
    # rank(P)=1: project to H_par
    """
    if mode == "hbot":
        newh = hs @ P + other_hs @ I_P
    elif mode == "hpar":
        newh = hs @ I_P + other_hs @ P
    else:
        raise ValueError(f"Incorrect mode {mode}")
    return newh

def sample_other_hs(other_hs, msamples, device):
    """ Samples msamples from other_hs with replacement.
    Output: other_hs_sample (msamples x d)
    """
    idx = np.random.randint(
        0, other_hs.shape[0], 
        msamples 
    )
    other_hs_sample = other_hs[idx].to(device)
    return other_hs_sample

def intervene_first_h(cxt_hidden_state, method, msamples, gen_all_hs, P, I_P, device):
    # n_ntok_h: 1 x d
    # first_hs: msamples x d
    first_hs = cxt_hidden_state.repeat(msamples, 1)

    # sampling other hs, shape (msamples x d)
    sampled_hs = sample_other_hs(
       gen_all_hs, msamples, device
    )

    # intervention on first hs
    # first_hs_int: shape (msamples x d)
    first_hs_int = apply_projection(
        first_hs, sampled_hs, method, P, I_P
    )
    return first_hs_int


